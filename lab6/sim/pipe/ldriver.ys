#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
# Zhang Yuandi
# 517030910230
# Describe how and why you modified the baseline code.
# 1. use iaddq -> Average CPE	12.70
# 2. loop unrolling 8 -> Average CPE	9.16
# 3. loop unrolling 8 4 2 1 -> Average CPE	9.00
# 4. loop unrolling >=8 -> >=4 -> >=2 -> 1  Remove unnecessary parts of jump -> Average CPE	8.24
# 5. reduce the total time of using iaddq by changing order -> Average CPE	8.01
# 6. improve add by avoiding repeated iaddq $1,%rax  ->  Average CPE	7.87
# 7. improve the remaining part  ->  Average CPE		7.69
# 8. remove xorq %rax,%rax  ->  Average CPE	7.64
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
	# Loop header

    iaddq $-8, %rdx
    jl Remain

unrollingLoop8:
    mrmovq (%rdi),%r8
    mrmovq 0x8(%rdi),%r9
    mrmovq 0x10(%rdi),%r10
    mrmovq 0x18(%rdi),%r11
    mrmovq 0x20(%rdi),%r12
    mrmovq 0x28(%rdi),%r13
    mrmovq 0x30(%rdi),%r14
    mrmovq 0x38(%rdi),%rcx

	andq %r8, %r8
	jle Eight_0 
#1
Eight_1:
    andq %r9, %r9
    jle Eight_01
Eight_11:
    andq %r10, %r10
    jle Eight_011
Eight_111:
    andq %r11, %r11
    jle Eight_0111
Eight_1111:
    andq %r12, %r12
    jle Eight_01111
Eight_11111:
    andq %r13, %r13
    jle Eight_011111
Eight_111111:
    andq %r14, %r14
    jle Eight_0111111
Eight_1111111:
    andq %rcx, %rcx
    jle count7 
    iaddq $8, %rax
    jmp Eight8

count7:
    iaddq $7, %rax
    jmp Eight8
#2
Eight_0:
    andq %r9, %r9
    jle Eight_00
Eight_01:
    andq %r10, %r10
    jle Eight_001
Eight_011:
    andq %r11, %r11
    jle Eight_0011
Eight_0111:
    andq %r12, %r12
    jle Eight_00111
Eight_01111:
    andq %r13, %r13
    jle Eight_001111
Eight_011111:
    andq %r14, %r14
    jle Eight_0011111
Eight_0111111:
    andq %rcx, %rcx 
    jg count7
count6:
    iaddq $6, %rax
    jmp Eight8
#3
Eight_00:
    andq %r10, %r10
    jle Eight_000
Eight_001:
    andq %r11, %r11
    jle Eight_0001
Eight_0011:
    andq %r12, %r12
    jle Eight_00011
Eight_00111:
    andq %r13, %r13
    jle Eight_000111
Eight_001111:
    andq %r14, %r14
    jle Eight_0001111
Eight_0011111:
    andq %rcx, %rcx
    jg count6
count5:
    iaddq $5, %rax
    jmp Eight8
#4
Eight_000:
    andq %r11, %r11
    jle Eight_0000
Eight_0001:
    andq %r12, %r12
    jle Eight_00001
Eight_00011:
    andq %r13, %r13
    jle Eight_000011
Eight_000111:
    andq %r14, %r14
    jle Eight_0000111
Eight_0001111:
    andq %rcx, %rcx
    jg count5
count4:
    iaddq $4, %rax
    jmp Eight8
#5
Eight_0000:
    andq %r12, %r12
    jle Eight_00000
Eight_00001:
    andq %r13, %r13
    jle Eight_000001
Eight_000011:
    andq %r14, %r14
    jle Eight_0000011
Eight_0000111:
    andq %rcx, %rcx
    jg count4
count3:
    iaddq $3, %rax
    jmp Eight8
#6
Eight_00000:
    andq %r13, %r13
    jle Eight_000000
Eight_000001:
    andq %r14, %r14
    jle Eight_0000001
Eight_0000011:
    andq %rcx, %rcx
    jg count3
count2:
    iaddq $2, %rax
    jmp Eight8
#7
Eight_000000:
    andq %r14, %r14
    jle Eight_0000000
Eight_0000001:
    andq %rcx, %rcx
    jg count2
count1:
    iaddq $1, %rax
    jmp Eight8
#8
Eight_0000000:
    andq %rcx, %rcx
    jg count1

Eight8:
    rmmovq %r8,(%rsi)
    rmmovq %r9,0x8(%rsi)
    rmmovq %r10,0x10(%rsi)
    rmmovq %r11,0x18(%rsi)
    rmmovq %r12,0x20(%rsi)
    rmmovq %r13,0x28(%rsi)
    rmmovq %r14,0x30(%rsi)
    rmmovq %rcx,0x38(%rsi)
    
	iaddq $64, %rdi		# src += 64
	iaddq $64, %rsi		# dst += 64
    iaddq $-8, %rdx
    jge unrollingLoop8

#0~7
Remain:
	iaddq $6, %rdx 
	jl Out

unrollingLoop2:
	mrmovq (%rdi), %r8
	mrmovq 8(%rdi), %r9
	andq %r8, %r8
	jle unrollingLoop
	iaddq $1, %rax

unrollingLoop:
	andq %r9, %r9
	jle Npos
	iaddq $1, %rax

Npos:
	rmmovq %r8, (%rsi)
	rmmovq %r9, 8(%rsi)
	iaddq $16, %rdi
	iaddq $16, %rsi
	iaddq $-2, %rdx
	jge unrollingLoop2

#0-1
Out:
	iaddq $1, %rdx
	jl Done
    mrmovq (%rdi), %r8
	andq %r8, %r8
    rmmovq %r8, (%rsi)
    jle Done
    iaddq $1, %rax

##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function

End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad -1
	.quad 2
	.quad -3
	.quad -4
	.quad -5
	.quad 6
	.quad 7
	.quad -8
	.quad -9
	.quad 10
	.quad 11
	.quad -12
	.quad -13
	.quad -14
	.quad 15
	.quad -16
	.quad -17
	.quad 18
	.quad -19
	.quad -20
	.quad 21
	.quad 22
	.quad 23
	.quad -24
	.quad -25
	.quad 26
	.quad -27
	.quad 28
	.quad 29
	.quad 30
	.quad -31
	.quad -32
	.quad -33
	.quad -34
	.quad -35
	.quad -36
	.quad -37
	.quad -38
	.quad -39
	.quad -40
	.quad -41
	.quad -42
	.quad 43
	.quad 44
	.quad 45
	.quad -46
	.quad -47
	.quad 48
	.quad 49
	.quad -50
	.quad 51
	.quad -52
	.quad 53
	.quad 54
	.quad 55
	.quad 56
	.quad 57
	.quad 58
	.quad 59
	.quad 60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
